{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kev-in-xu/Phylo-Exceptions/blob/main/Organisms_lacking_Dmc1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOCKuzdIH-bR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "!pip install biopython\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from Bio import SeqIO\n",
        "from Bio import Entrez\n",
        "\n",
        "import requests\n",
        "\n",
        "#%env STORAGE_FILE_PATH=/content/drive/MyDrive/Greene_lab/\n",
        "%env STORAGE_FILE_PATH=/content/drive/MyDrive/Greene-Lab/lacking_organisms/nov_data/\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# ! ls $STORAGE_FILE_PATH\n",
        "\n",
        "\n",
        "Entrez.api_key = \"a5d84a29ce91f0aa01001ad64a26fc145208\"\n",
        "Entrez.email = \"kkx2102@columbia.edu\"\n",
        "path = %env STORAGE_FILE_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmQq9L5TIz02"
      },
      "source": [
        "### Workbook upload / parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j0V2FZtJC9Q"
      },
      "outputs": [],
      "source": [
        "path = %env STORAGE_FILE_PATH\n",
        "rad51 = pd.read_excel(path + \"Rad51_Workbook.xlsx\")\n",
        "dmc1 = pd.read_excel(path + \"Dmc1_Workbook.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GbajaenLa8G"
      },
      "outputs": [],
      "source": [
        "f = open(path + \"Dmc1_seqdump.txt\", \"r\")\n",
        "sequences = []\n",
        "current_sequence = {\"header\": \"\", \"sequence\": \"\"}\n",
        "\n",
        "# Open and read the FASTA file\n",
        "with open(path + \"Dmc1_seqdump.txt\", \"r\") as fasta_file:\n",
        "    for line in fasta_file:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):  # Header line\n",
        "            if current_sequence[\"header\"]:\n",
        "                sequences.append(current_sequence.copy())\n",
        "            current_sequence[\"header\"] = line[1:]  # Remove '>' from header\n",
        "            current_sequence[\"sequence\"] = \"\"\n",
        "        else:  # Sequence line\n",
        "            current_sequence[\"sequence\"] += line\n",
        "\n",
        "# Don't forget to add the last sequence after the loop\n",
        "if current_sequence[\"header\"]:\n",
        "    sequences.append(current_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M080s00KMAhU"
      },
      "outputs": [],
      "source": [
        "Dmc1_orgs = []\n",
        "\n",
        "for seq in sequences:\n",
        "  for a in seq[\"header\"].split():\n",
        "    if a.startswith(\"[\"):\n",
        "      Dmc1_orgs.append(a[1:])\n",
        "\n",
        "dmc1_genus = list(set([org for org in Dmc1_orgs]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTPuOZn5Q7kw"
      },
      "source": [
        "### pBLAST Automation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tBnmOY9Q6e6"
      },
      "outputs": [],
      "source": [
        "### edited from ChatGPT\n",
        "\n",
        "yeast_seq = \"MSVTGTEIDSDTAKNILSVDELQNYGINASDLQKLKSGGIYTVNTVLSTTRRHLCKIKGLSEVKVEKIKEAAGKIIQVGFIPATVQLDIRQRVYSLSTGSKQLDSILGGGIMTMSITEVFGEFRCGKTQMSHTLCVTTQLPREMGGGEGKVAYIDTEGTFRPERIKQIAEGYELDPESCLANVSYARALNSEHQMELVEQLGEELSSGDYRLIVVDSIMANFRVDYCGRGELSERQQKLNQHLFKLNRLAEEFNVAVFLTNQVQSDPGASALFASADGRKPIGGHVLAHASATRILLRKGRGDERVAKLQDSPDMPEKECVYVIGEKGITDSSD\"\n",
        "\n",
        "human_seq = \"MKEDQVVAEEPGFQDEEESLFQDIDLLQKHGINVADIKKLKSVGICTIKGIQMTTRRALCNVKGLSEAKVDKIKEAANKLIEPGFLTAFEYSEKRKMVFHITTGSQEFDKLLGGGIESMAITEAFGEFRTGKTQLSHTLCVTAQLPGAGGYPGGKIIFIDTENTFRPDRLRDIADRFNVDHDAVLDNVLYARAYTSEHQMELLDYVAAKFHEEAGIFKLLIIDSIMALFRVDFSGRGELAERQQKLAQMLSRLQKISEEYNVAVFVTNQMTADPGATMTFQADPKKPIGGHILAHASTTRISLRKGRGELRIAKIYDSPEMPENEATFAITAGGIGDAKE\"\n",
        "\n",
        "# Replace this with your actual query sequence in FASTA format\n",
        "# query_seq = \">Query\\n\" + yeast_seq\n",
        "query_seq = \">Query\\n\" + human_seq\n",
        "\n",
        "# Perform pBLAST search\n",
        "result_handle = NCBIWWW.qblast(\"blastp\", \"refseq_protein\", query_seq, hitlist_size=500)\n",
        "\n",
        "# Parse the BLAST results\n",
        "blast_records = NCBIXML.parse(result_handle)\n",
        "\n",
        "# List to store organisms without DMC1 or HSLIM15 entry\n",
        "organisms_without_entry = []\n",
        "organisms = []\n",
        "\n",
        "# Iterate through BLAST records\n",
        "for record in blast_records:\n",
        "    for alignment in record.alignments:\n",
        "        organisms.append(alignment.title.split(\"[\")[1].split(\"]\")[0])\n",
        "\n",
        "#alignment_title = alignment.title\n",
        "# Check if alignment title contains DMC1 or HSLIM15\n",
        "#if \"DMC1\" not in alignment_title.upper() and \"HSLIM15\" not in alignment_title.upper():\n",
        " #   organism = alignment_title.split(\"[\")[1].split(\"]\")[0]\n",
        "  #  organisms_without_entry.append(organism)\n",
        "\n",
        "\n",
        "# Print organisms without DMC1/HSLIM15 entry\n",
        "print(\"Organisms without DMC1 or HSLIM15 entry:\")\n",
        "for organism in organisms_without_entry:\n",
        "    print(organism)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJFXPYpzrIkE"
      },
      "outputs": [],
      "source": [
        "len(record.alignments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXw-1oniq0l2"
      },
      "outputs": [],
      "source": [
        "alignment.hsps[0].expect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryXPc0hlaORw"
      },
      "outputs": [],
      "source": [
        "organisms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slgJTtpeKDPG"
      },
      "source": [
        "### BLASTing exceptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pUbZRmqKK8r"
      },
      "outputs": [],
      "source": [
        "#path = %env STORAGE_FILE_PATH\n",
        "# opens excel files\n",
        "#rad51 = pd.read_excel(path + \"Rad51_Workbook.xlsx\")\n",
        "# opens files\n",
        "#f = open(path + \"Dmc1_seqdump.txt\", \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rXrnpsCKYxx"
      },
      "outputs": [],
      "source": [
        "path = %env STORAGE_FILE_PATH\n",
        "Dmc1branch = open(path + \"Dmc1branch_240-520_85.fa.clstr\", \"r\")\n",
        "\n",
        "pblastSeqs = []\n",
        "current_sequence = {\"header\": \"\",\"accession no.\": \"\",\n",
        "                    \"organism\": \"\", \"sequence\": \"\", \"length\": \"\"}\n",
        "\n",
        "# Open and read the pBLAST FASTA file\n",
        "with open(path + \"Rad51_1e-25_7693.fasta\", \"r\") as pblastResult:\n",
        "    for line in pblastResult:\n",
        "        line = line.strip()\n",
        "        if line.startswith(\">\"):  # Header line\n",
        "            if current_sequence[\"header\"]:\n",
        "              current_sequence[\"length\"] = len(current_sequence[\"sequence\"])\n",
        "              pblastSeqs.append(current_sequence.copy())\n",
        "            current_sequence[\"header\"] = line[1:]  # Remove '>' from header\n",
        "            current_sequence[\"sequence\"] = \"\"\n",
        "            current_sequence[\"organism\"] = line[line.find(\"[\")+1:line.rfind(\"]\")]\n",
        "            current_sequence[\"accession no.\"] = line[1:line.find(\" \")]\n",
        "        else:  # Sequence line\n",
        "            current_sequence[\"sequence\"] += line\n",
        "\n",
        "# Don't forget to add the last sequence after the loop\n",
        "if current_sequence[\"header\"]:\n",
        "    pblastSeqs.append(current_sequence)\n",
        "\n",
        "master = pd.DataFrame.from_dict(pblastSeqs)\n",
        "\n",
        "\n",
        "###DMC1###\n",
        "Dmc1Clusters = []\n",
        "current_cluster = {\"accession no.\": \"\", \"protein\": \"\"}\n",
        "\n",
        "# Open and read the Dmc1 FASTA file\n",
        "with open(path + \"Dmc1branch_240-520_85.fa.clstr\", \"r\") as Dmc1File:\n",
        "    for line in Dmc1File:\n",
        "        line = line.strip()\n",
        "        if not line.startswith(\">\"):  # non-cluster line\n",
        "            current_cluster[\"accession no.\"] = line[line.find(\">\")+1:line.find(\".\")]\n",
        "            current_cluster[\"protein\"] = \"Dmc1\"\n",
        "            Dmc1Clusters.append(current_cluster.copy())\n",
        "Dmc1 = pd.DataFrame.from_dict(Dmc1Clusters)\n",
        "\n",
        "\n",
        "###RAD51###\n",
        "Rad51Clusters = []\n",
        "current_cluster = {\"accession no.\": \"\", \"protein\": \"\"}\n",
        "\n",
        "# Open and read the Dmc1 FASTA file\n",
        "with open(path + \"Rad51branch_240-520_85.fa.clstr\", \"r\") as Rad51File:\n",
        "    for line in Rad51File:\n",
        "        line = line.strip()\n",
        "        if not line.startswith(\">\"):  # non-cluster line\n",
        "            current_cluster[\"accession no.\"] = line[line.find(\">\")+1:line.find(\".\")]\n",
        "            current_cluster[\"protein\"] = \"Rad51\"\n",
        "            Rad51Clusters.append(current_cluster.copy())\n",
        "Rad51 = pd.DataFrame.from_dict(Rad51Clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKHNVCQOZirU"
      },
      "outputs": [],
      "source": [
        "# Merges Columns to include protein data\n",
        "master=master.merge(Rad51, how=\"left\", left_on=\"accession no.\", right_on=\"accession no.\")\n",
        "master=master.merge(Dmc1, how=\"left\", left_on=\"accession no.\", right_on=\"accession no.\")\n",
        "\n",
        "def combine_columns(row):\n",
        "    return row[\"protein_x\"] if pd.isna(row[\"protein_y\"]) else row[\"protein_y\"]\n",
        "\n",
        "master['protein'] = master.apply(combine_columns, axis=1)\n",
        "master.drop(columns=[\"protein_x\",\"protein_y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCuDjn5qeFbD"
      },
      "outputs": [],
      "source": [
        "Rad51_orgs = list(master[\"organism\"][master[\"protein\"] == \"Rad51\"])\n",
        "Dmc1_orgs = list(master[\"organism\"][master[\"protein\"] == \"Dmc1\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjJM91uQffEp"
      },
      "outputs": [],
      "source": [
        "Rad51_orgs = list(set([org.split(\" \")[0] for org in Rad51_orgs]))\n",
        "Dmc1_orgs = list(set([org.split(\" \")[0] for org in Dmc1_orgs]))\n",
        "\n",
        "Rad51_only = [org for org in Rad51_orgs if org not in Dmc1_orgs]\n",
        "Dmc1_only = [org for org in Dmc1_orgs if org not in Rad51_orgs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55BLuqMZjb1p"
      },
      "outputs": [],
      "source": [
        "Rad51_only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvUBf5xO2Ix4"
      },
      "source": [
        "#### Mike's Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06U_u182ZmVO"
      },
      "outputs": [],
      "source": [
        "###RAD51###\n",
        "Rad51Clusters = []\n",
        "current_cluster = {\"accession no.\": \"\", \"protein\": \"\"}\n",
        "\n",
        "# Open and read the Dmc1 FASTA file\n",
        "with open(path + \"Rad51branch_240-520_85.fa.clstr\", \"r\") as Rad51File:\n",
        "    for line in Rad51File:\n",
        "        line = line.strip()\n",
        "        if not line.startswith(\">\"):  # non-cluster line\n",
        "            current_cluster[\"accession no.\"] = line[line.find(\">\")+1:line.find(\".\")]\n",
        "            current_cluster[\"protein\"] = \"Rad51\"\n",
        "            Rad51Clusters.append(current_cluster.copy())\n",
        "Rad51 = pd.DataFrame.from_dict(Rad51Clusters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZChJ4kiPcnJH"
      },
      "source": [
        "### Just using NCBI Taxonomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuAYyFlMZnpi"
      },
      "outputs": [],
      "source": [
        "### PARSING ALL PHYLOGENETIC ORDERS FROM GIVEN FILES ###\n",
        "path = %env STORAGE_FILE_PATH\n",
        "orders = []\n",
        "\n",
        "with open(path + \"Metazoa_orders.txt\", \"r\") as taxonomy:\n",
        "  for line in taxonomy:\n",
        "    line = line.strip()\n",
        "    if line.find(\"-\") != -1 and line.find(\"---\") == -1:\n",
        "      orders.append(line[line.find(\"-\")+1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH7VMo35v9QV"
      },
      "outputs": [],
      "source": [
        "##### GETS TAXIDS FROM LIST OF TAXONOMY RANKS ######\n",
        "\n",
        "IDs = []\n",
        "ID = {\"organism\": \"\", \"txid\": \"\", \"count\": \"\"}\n",
        "\n",
        "# Iterates through organisms\n",
        "for organism_name in orders:\n",
        "  # Construct the URL for the NCBI Taxonomy API\n",
        "  base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "  search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name\n",
        "\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(search_url)\n",
        "  data = response.text\n",
        "\n",
        "  # Extract the TaxID from the response\n",
        "  if data.find(\"<Id>\")!=-1:\n",
        "    taxid = data[data.find(\"<Id>\")+4:data.find(\"</Id>\")]\n",
        "    print(f\"TaxID for {organism_name}: {taxid}\")\n",
        "  else:\n",
        "    taxid = \"\"\n",
        "    #print(f\"TaxID not found for {organism_name}\")\n",
        "\n",
        "  ID[\"organism\"]=organism_name\n",
        "  ID[\"txid\"]=taxid + \"[ORGN]\"\n",
        "\n",
        "  # Construct the URL for the NCBI Taxonomy API\n",
        "  base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "  search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name + \"[Organism]\"\n",
        "\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(search_url)\n",
        "  data = response.text\n",
        "\n",
        "  # Extract the TaxID from the response\n",
        "  if data.find(\"<Count>\")!=-1:\n",
        "    count = data[data.find(\"<Count>\")+7:data.find(\"</Count>\")]\n",
        "    print(f\"Count for {organism_name}: {count}\")\n",
        "  else:\n",
        "    count = 0\n",
        "    #print(f\"TaxID not found for {organism_name}\")\n",
        "\n",
        "  ID[\"count\"]=count\n",
        "  IDs.append(ID.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ6EhhKcFLGS"
      },
      "outputs": [],
      "source": [
        "IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnc81vAMsPvN"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name\n",
        "\n",
        "# Send a GET request to the API\n",
        "response = requests.get(search_url)\n",
        "data = response.text\n",
        "\n",
        "# Extract the TaxID from the response\n",
        "if data.find(\"<Id>\")!=-1:\n",
        "  taxid = data[data.find(\"<Id>\")+4:data.find(\"</Id>\")]\n",
        "  print(f\"TaxID for {organism_name}: {taxid}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAtoeFSxdftB"
      },
      "outputs": [],
      "source": [
        "### Exports Taxids to csv ####\n",
        "import csv\n",
        "\n",
        "field_names = ['organism', 'txid', 'count']\n",
        "\n",
        "with open(path +'Names.csv', 'w') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(IDs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K35d1dJ3HMkk"
      },
      "source": [
        "#### Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUSwGeoydLlm"
      },
      "outputs": [],
      "source": [
        "### Reads existing csv for dictionary ###\n",
        "\n",
        "path = %env STORAGE_FILE_PATH\n",
        "\n",
        "IDs = []\n",
        "with open(path +'Names.csv', 'r') as csvfile:\n",
        "  csv_reader = csv.DictReader(csvfile)\n",
        "  for row in csv_reader:\n",
        "    IDs.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stPW-YLwgB0F"
      },
      "outputs": [],
      "source": [
        "zeroes = []\n",
        "\n",
        "for org in IDs:\n",
        "  if int(org[\"count\"]) == 0 or org[\"txid\"] == \"[ORGN]\":\n",
        "    zeroes.append(org)\n",
        "\n",
        "\n",
        "zero_orgs = []\n",
        "\n",
        "for org in zeroes:\n",
        "  zero_orgs.append(org[\"organism\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPfxwr6AgbdH"
      },
      "outputs": [],
      "source": [
        "len(zero_orgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8zvT3rMg11r"
      },
      "outputs": [],
      "source": [
        "##### GETS TAXIDS FROM LIST OF TAXONOMY RANKS ######\n",
        "\n",
        "import time\n",
        "\n",
        "IDs = []\n",
        "ID = {\"organism\": \"\", \"txid\": \"\", \"count\": \"\"}\n",
        "\n",
        "# Iterates through organisms\n",
        "for organism_name in zero_orgs:\n",
        "  # Construct the URL for the NCBI Taxonomy API\n",
        "  base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "  search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name\n",
        "\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(search_url)\n",
        "  data = response.text\n",
        "\n",
        "  # Extract the TaxID from the response\n",
        "  if data.find(\"<Id>\")!=-1:\n",
        "    taxid = data[data.find(\"<Id>\")+4:data.find(\"</Id>\")]\n",
        "    print(f\"TaxID for {organism_name}: {taxid}\")\n",
        "  else:\n",
        "    taxid = \"\"\n",
        "    print(f\"TaxID not found for {organism_name}\")\n",
        "\n",
        "  ID[\"organism\"]=organism_name\n",
        "  ID[\"txid\"]=taxid + \"[ORGN]\"\n",
        "\n",
        "  # Construct the URL for the NCBI Taxonomy API\n",
        "  base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "  search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name + \"[Organism]\"\n",
        "\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(search_url)\n",
        "  data = response.text\n",
        "\n",
        "  # Extract the TaxID from the response\n",
        "  if data.find(\"<Count>\")!=-1:\n",
        "    count = data[data.find(\"<Count>\")+7:data.find(\"</Count>\")]\n",
        "    print(f\"Count for {organism_name}: {count}\")\n",
        "  else:\n",
        "    count = 0\n",
        "    print(f\"Count not found for {organism_name}\")\n",
        "\n",
        "  ID[\"count\"]=count\n",
        "  IDs.append(ID.copy())\n",
        "  #time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_AUlyvKia0N"
      },
      "outputs": [],
      "source": [
        "IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA3__KWk9Vcf"
      },
      "outputs": [],
      "source": [
        "IDs[590:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdyKm_0Gsu8g"
      },
      "outputs": [],
      "source": [
        "yeast_seq = \"MSVTGTEIDSDTAKNILSVDELQNYGINASDLQKLKSGGIYTVNTVLSTTRRHLCKIKGLSEVKVEKIKEAAGKIIQVGFIPATVQLDIRQRVYSLSTGSKQLDSILGGGIMTMSITEVFGEFRCGKTQMSHTLCVTTQLPREMGGGEGKVAYIDTEGTFRPERIKQIAEGYELDPESCLANVSYARALNSEHQMELVEQLGEELSSGDYRLIVVDSIMANFRVDYCGRGELSERQQKLNQHLFKLNRLAEEFNVAVFLTNQVQSDPGASALFASADGRKPIGGHVLAHASATRILLRKGRGDERVAKLQDSPDMPEKECVYVIGEKGITDSSD\"\n",
        "\n",
        "human_seq = \"MKEDQVVAEEPGFQDEEESLFQDIDLLQKHGINVADIKKLKSVGICTIKGIQMTTRRALCNVKGLSEAKVDKIKEAANKLIEPGFLTAFEYSEKRKMVFHITTGSQEFDKLLGGGIESMAITEAFGEFRTGKTQLSHTLCVTAQLPGAGGYPGGKIIFIDTENTFRPDRLRDIADRFNVDHDAVLDNVLYARAYTSEHQMELLDYVAAKFHEEAGIFKLLIIDSIMALFRVDFSGRGELAERQQKLAQMLSRLQKISEEYNVAVFVTNQMTADPGATMTFQADPKKPIGGHILAHASTTRISLRKGRGELRIAKIYDSPEMPENEATFAITAGGIGDAKE\"\n",
        "\n",
        "\n",
        "# List to store organisms without DMC1 or HSLIM15 entry\n",
        "containsDmc1 = []\n",
        "lacksDmc1 = []\n",
        "fewerThan100 = []\n",
        "\n",
        "for ID in IDs[599:600]:\n",
        "  hasDmc1 = False\n",
        "\n",
        "  if int(ID[\"count\"]) > 100:\n",
        "    # Replace this with your actual query sequence in FASTA format\n",
        "    # query_seq = \">Query\\n\" + yeast_seq\n",
        "    query_seq = \">Query\\n\" + human_seq\n",
        "    order = ID[\"organism\"] + \"[ORGN]\"\n",
        "\n",
        "    # Perform pBLAST search\n",
        "    result = NCBIWWW.qblast(\"blastp\", \"nr\", query_seq, entrez_query=order, hitlist_size=10)\n",
        "\n",
        "    # Parse the BLAST results\n",
        "    rad51_parsed = NCBIXML.parse(result)\n",
        "\n",
        "    # Iterate through BLAST records\n",
        "    for record in rad51_parsed:\n",
        "      for alignment in record.alignments:\n",
        "        if (alignment.title.find(\"dmc1\") != -1 or alignment.title.find(\"Dmc1\") != -1\n",
        "            or alignment.title.find(\"DMC1\") != -1or alignment.title.find(\"lim15\") != -1\n",
        "            or alignment.title.find(\"Lim15\") != -1 or alignment.title.find(\"LIM15\") != -1\n",
        "            or alignment.hsps[0].expect < 1e-120): # THIS IS THE FILTER CRITERIA\n",
        "          hasDmc1 = True\n",
        "      if hasDmc1:\n",
        "        containsDmc1.append(ID[\"organism\"])\n",
        "        print(ID[\"organism\"] + \" has Dmc1\")\n",
        "      else:\n",
        "        lacksDmc1.append(ID[\"organism\"])\n",
        "        print(ID[\"organism\"] + \" lack Dmc1!\")\n",
        "  else:\n",
        "    fewerThan100.append(ID[\"organism\"])\n",
        "    print(ID[\"organism\"] + \" has fewer than 100 entries\")\n",
        "\n",
        "with open(path + \"output.txt\", \"w\") as output:\n",
        "  output.write('-----containsDmc1-----\\n')\n",
        "  for order in containsDmc1:\n",
        "    output.write(order + \"\\n\")\n",
        "  output.write('\\n-----lacksDmc1-----\\n')\n",
        "  for order in lacksDmc1:\n",
        "    output.write(order + \"\\n\")\n",
        "  output.write('\\n-----fewerThan100-----\\n')\n",
        "  for order in fewerThan100:\n",
        "    output.write(str(order) + \"\\n\")\n",
        "\n",
        "files.download(\"output.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVveXr0FChhs"
      },
      "source": [
        "## October Plan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A64AkjanCuo-"
      },
      "outputs": [],
      "source": [
        "### Reads existing csv for dictionary ###\n",
        "\n",
        "path = %env STORAGE_FILE_PATH\n",
        "\n",
        "orders = []\n",
        "with open(path +'plant_orders.csv', 'r') as csvfile:\n",
        "  csv_reader = csv.DictReader(csvfile)\n",
        "  for row in csv_reader:\n",
        "    orders.append(row[\"Order\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz1EedfLEaFJ"
      },
      "outputs": [],
      "source": [
        "orders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6w--Ky1CkX8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "IDs = []\n",
        "ID = {\"organism\": \"\", \"txid\": \"\"}\n",
        "\n",
        "# Iterates through organisms\n",
        "for organism_name in orders:\n",
        "  time.sleep(0.25)\n",
        "\n",
        "  # Construct the URL for the NCBI Taxonomy API\n",
        "  base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "  search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name\n",
        "\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(search_url)\n",
        "  data = response.text\n",
        "\n",
        "  # Extract the TaxID from the response\n",
        "  if data.find(\"<Id>\")!=-1:\n",
        "    taxid = data[data.find(\"<Id>\")+4:data.find(\"</Id>\")]\n",
        "    print(f\"TaxID for {organism_name}: {taxid}\")\n",
        "  else:\n",
        "    # Construct the URL for the NCBI Taxonomy API\n",
        "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
        "    search_url = base_url + \"esearch.fcgi?db=taxonomy&term=\" + organism_name\n",
        "\n",
        "    # Send a GET request to the API\n",
        "    response = requests.get(search_url)\n",
        "    data = response.text\n",
        "\n",
        "    # Extract the TaxID from the response\n",
        "    if data.find(\"<Id>\")!=-1:\n",
        "      taxid = data[data.find(\"<Id>\")+4:data.find(\"</Id>\")]\n",
        "      print(f\"TaxID for {organism_name}: {taxid}\")\n",
        "    else:\n",
        "      taxid = \"\"\n",
        "      print(f\"TaxID not found for {organism_name}\")\n",
        "\n",
        "  ID[\"organism\"]=organism_name\n",
        "  ID[\"txid\"]=taxid + \"[ORGN]\"\n",
        "\n",
        "  IDs.append(ID.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HipTLnTrPUTd"
      },
      "outputs": [],
      "source": [
        "IDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0bMGx5TPOJc"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "field_names = ['organism', 'txid']\n",
        "\n",
        "with open(path +'plant_orders_txid.csv', 'w') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames = field_names)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28LXWH78PaQ6"
      },
      "outputs": [],
      "source": [
        "files.download(path +'plant_orders_txid.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrz0awbfLiZ8"
      },
      "source": [
        "### Txid CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBUcU9Wfp0ov"
      },
      "outputs": [],
      "source": [
        "### Reads existing csv for dictionary ###\n",
        "\n",
        "path = %env STORAGE_FILE_PATH\n",
        "\n",
        "IDs = []\n",
        "order = {\"organism\":\"\",\"txid\":\"\"}\n",
        "with open(path +'plant_orders_txid.csv', 'r') as csvfile:\n",
        "  csv_reader = csv.DictReader(csvfile)\n",
        "  for row in csv_reader:\n",
        "    order[\"organism\"] = row[\"organism\"]\n",
        "    order[\"txid\"] = row[\"txid\"]\n",
        "    IDs.append(order.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okvHyNqfPQQE"
      },
      "outputs": [],
      "source": [
        "len(IDs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI1uMydMHHgC"
      },
      "outputs": [],
      "source": [
        "# List to store organisms without DMC1 or HSLIM15 entry\n",
        "containsDmc1 = []\n",
        "lacksDmc1 = []\n",
        "fewerThan100 = []\n",
        "\n",
        "yeast_seq = \"MSVTGTEIDSDTAKNILSVDELQNYGINASDLQKLKSGGIYTVNTVLSTTRRHLCKIKGLSEVKVEKIKEAAGKIIQVGFIPATVQLDIRQRVYSLSTGSKQLDSILGGGIMTMSITEVFGEFRCGKTQMSHTLCVTTQLPREMGGGEGKVAYIDTEGTFRPERIKQIAEGYELDPESCLANVSYARALNSEHQMELVEQLGEELSSGDYRLIVVDSIMANFRVDYCGRGELSERQQKLNQHLFKLNRLAEEFNVAVFLTNQVQSDPGASALFASADGRKPIGGHVLAHASATRILLRKGRGDERVAKLQDSPDMPEKECVYVIGEKGITDSSD\"\n",
        "\n",
        "human_seq = \"MKEDQVVAEEPGFQDEEESLFQDIDLLQKHGINVADIKKLKSVGICTIKGIQMTTRRALCNVKGLSEAKVDKIKEAANKLIEPGFLTAFEYSEKRKMVFHITTGSQEFDKLLGGGIESMAITEAFGEFRTGKTQLSHTLCVTAQLPGAGGYPGGKIIFIDTENTFRPDRLRDIADRFNVDHDAVLDNVLYARAYTSEHQMELLDYVAAKFHEEAGIFKLLIIDSIMALFRVDFSGRGELAERQQKLAQMLSRLQKISEEYNVAVFVTNQMTADPGATMTFQADPKKPIGGHILAHASTTRISLRKGRGELRIAKIYDSPEMPENEATFAITAGGIGDAKE\"\n",
        "\n",
        "\n",
        "for ID in IDs[74:]:\n",
        "  hasDmc1 = False\n",
        "\n",
        "  # Replace this with your actual query sequence in FASTA format\n",
        "  query_seq = \">Query\\n\" + yeast_seq\n",
        "  #query_seq = \">Query\\n\" + human_seq\n",
        "  order = ID[\"organism\"] + \"[ORGN]\"\n",
        "\n",
        "  # Perform pBLAST search\n",
        "  result = NCBIWWW.qblast(\"blastp\", \"nr\", query_seq, entrez_query=order, hitlist_size=10)\n",
        "\n",
        "  # Parse the BLAST results\n",
        "  rad51_parsed = NCBIXML.parse(result)\n",
        "\n",
        "  # Iterate through BLAST records\n",
        "  for record in rad51_parsed:\n",
        "    for alignment in record.alignments:\n",
        "      if (alignment.title.find(\"dmc1\") != -1 or alignment.title.find(\"Dmc1\") != -1\n",
        "          or alignment.title.find(\"DMC1\") != -1 or alignment.title.find(\"lim15\") != -1\n",
        "          or alignment.title.find(\"Lim15\") != -1 or alignment.title.find(\"LIM15\") != -1\n",
        "          or alignment.hsps[0].expect < 1e-120): # THIS IS THE FILTER CRITERIA\n",
        "        hasDmc1 = True\n",
        "    if hasDmc1:\n",
        "      containsDmc1.append(ID[\"organism\"])\n",
        "      print(ID[\"organism\"] + \" has Dmc1\")\n",
        "    else:\n",
        "      lacksDmc1.append(ID[\"organism\"])\n",
        "      print(ID[\"organism\"] + \" lack Dmc1\")\n",
        "\n",
        "with open(path + \"output.txt\", \"w\") as output:\n",
        "  output.write('-----containsDmc1-----\\n')\n",
        "  for order in containsDmc1:\n",
        "    output.write(order + \"\\n\")\n",
        "  output.write('\\n-----lacksDmc1-----\\n')\n",
        "  for order in lacksDmc1:\n",
        "    output.write(order + \"\\n\")\n",
        "  output.write('\\n-----fewerThan100-----\\n')\n",
        "  for order in fewerThan100:\n",
        "    output.write(str(order) + \"\\n\")\n",
        "\n",
        "files.download(path + \"output.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fqUc7IpCNPv7"
      },
      "outputs": [],
      "source": [
        "### RAD51 CHECKLIST\n",
        "# List to store organisms without Rad51 entry\n",
        "containsRad51 = []\n",
        "lacksRad51 = []\n",
        "fewerThan100 = []\n",
        "\n",
        "yeast_Rad51 = \"MSQVQEQHISESQLQYGNGSLMSTVPADLSQSVVDGNGNGSSEDIEATNGSGDGGGLQEQAEAQGEMEDEAYDEAALGSFVPIEKLQVNGITMADVKKLRESGLHTAEAVAYAPRKDLLEIKGISEAKADKLLNEAARLVPMGFVTAADFHMRRSELICLTTGSKNLDTLLGGGVETGSITELFGEFRTGKSQLCHTLAVTCQIPLDIGGGEGKCLYIDTEGTFRPVRLVSIAQRFGLDPDDALNNVAYARAYNADHQLRLLDAAAQMMSESRFSLIVVDSVMALYRTDFSGRGELSARQMHLAKFMRALQRLADQFGVAVVVTNQVVAQVDGGMAFNPDPKKPIGGNIMAHSSTTRLGFKKGKGCQRLCKVVDSPCLPEAECVFAIYEDGVGDPREEDE\"\n",
        "human_Rad51 = \"MAMQMQLEANADTSVEEESFGPQPISRLEQCGINANDVKKLEEAGFHTVEAVAYAPKKELINIKGISEAKADKILAEAAKLVPMGFTTATEFHQRRSEIIQITTGSKELDKLLQGGIETGSITEMFGEFRTGKTQICHTLAVTCQLPIDRGGGEGKAMYIDTEGTFRPERLLAVAERYGLSGSDVLDNVAYARAFNTDHQTQLLYQASAMMVESRYALLIVDSATALYRTDYSGRGELSARQMHLARFLRMLLRLADEFGVAVVITNQVVAQVDGAAMFAADPKKPIGGNIIAHASTTRLYLRKGRGETRICKIYDSPCLPEAEAMFAINADGVGDAKD\"\n",
        "\n",
        "\n",
        "for ID in IDs[72:]:\n",
        "  hasRad51 = False\n",
        "\n",
        "  # Replace this with your actual query sequence in FASTA format\n",
        "  # query_seq = \">Query\\n\" + yeast_Rad51\n",
        "  query_seq = \">Query\\n\" + human_Rad51\n",
        "  order = ID[\"organism\"] + \"[ORGN]\"\n",
        "\n",
        "  # Perform pBLAST search\n",
        "  result = NCBIWWW.qblast(\"blastp\", \"nr\", query_seq, entrez_query=order, hitlist_size=10)\n",
        "\n",
        "  # Parse the BLAST results\n",
        "  rad51_parsed = NCBIXML.parse(result)\n",
        "\n",
        "  # Iterate through BLAST records\n",
        "  for record in rad51_parsed:\n",
        "    for alignment in record.alignments:\n",
        "      if (alignment.title.find(\"Rad51\") != -1 or alignment.title.find(\"RAD51\") != -1\n",
        "          or alignment.hsps[0].expect < 1e-120): # THIS IS THE FILTER CRITERIA\n",
        "        hasRad51 = True\n",
        "    if hasRad51:\n",
        "      containsRad51.append(ID[\"organism\"])\n",
        "      print(ID[\"organism\"] + \" has Rad51\")\n",
        "    else:\n",
        "      lacksRad51.append(ID[\"organism\"])\n",
        "      print(ID[\"organism\"] + \" lack Rad51!\")\n",
        "\n",
        "with open(path + \"output.txt\", \"w\") as output:\n",
        "  output.write('-----containsRad51-----\\n')\n",
        "  for order in containsRad51:\n",
        "    output.write(order + \"\\n\")\n",
        "  output.write('\\n-----lacksRad51-----\\n')\n",
        "  for order in lacksRad51:\n",
        "    output.write(order + \"\\n\")\n",
        "#  output.write('\\n-----fewerThan100-----\\n')\n",
        "#  for order in fewerThan100:\n",
        "#    output.write(str(order) + \"\\n\")\n",
        "\n",
        "files.download(path + \"output.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-uTdTZa9SlU"
      },
      "source": [
        "# November Plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-XT_zw79WEr"
      },
      "source": [
        "The plan is to download the top 3 sequences and headers of each species, do a manual sifting of results so that it becomes one sequence per order. Then do an alignment and FastTree pipeline to create the newick tree. Then put it into a pdf form and do the highlighting for visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_Hh28IQucxg"
      },
      "outputs": [],
      "source": [
        "### Reads existing csv for dictionary ###\n",
        "\n",
        "orders = []\n",
        "with open(path +'metazoa_orders_pruned.csv', 'r') as csvfile:\n",
        "  csv_reader = csv.DictReader(csvfile)\n",
        "  for row in csv_reader:\n",
        "    orders.append(row[\"Order\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arkr-EQHz9xz"
      },
      "outputs": [],
      "source": [
        "### Reads master csv in case progress is dropped ###\n",
        "\n",
        "master = pd.read_csv(path + \"metazoa_data_nov.csv\")\n",
        "master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghSBLZBjbUzS"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "          \"Order\": [\"temp\"],\n",
        "          \"Title\": [\"temp\"],\n",
        "          \"Alignment Length\": [0],\n",
        "          \"Alignment Sequence\": [\"temp\"]\n",
        "      }\n",
        "master = pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKEZjJ2D_Otc"
      },
      "outputs": [],
      "source": [
        "master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YEFGK7TYlyr"
      },
      "outputs": [],
      "source": [
        "orders[60:65]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JAd2ViZKmukn"
      },
      "outputs": [],
      "source": [
        "def fetch_protein_sequence(accession_number):\n",
        "    try:\n",
        "        # Fetch the protein record using the accession number\n",
        "        handle = Entrez.efetch(db=\"protein\", id=accession_number, rettype=\"fasta\", retmode=\"text\")\n",
        "        record = handle.read()\n",
        "        handle.close()\n",
        "        return record\n",
        "    except Exception as e:\n",
        "        return str(e)\n",
        "\n",
        "yeast_Rad51 = \"MSQVQEQHISESQLQYGNGSLMSTVPADLSQSVVDGNGNGSSEDIEATNGSGDGGGLQEQAEAQGEMEDEAYDEAALGSFVPIEKLQVNGITMADVKKLRESGLHTAEAVAYAPRKDLLEIKGISEAKADKLLNEAARLVPMGFVTAADFHMRRSELICLTTGSKNLDTLLGGGVETGSITELFGEFRTGKSQLCHTLAVTCQIPLDIGGGEGKCLYIDTEGTFRPVRLVSIAQRFGLDPDDALNNVAYARAYNADHQLRLLDAAAQMMSESRFSLIVVDSVMALYRTDFSGRGELSARQMHLAKFMRALQRLADQFGVAVVVTNQVVAQVDGGMAFNPDPKKPIGGNIMAHSSTTRLGFKKGKGCQRLCKVVDSPCLPEAECVFAIYEDGVGDPREEDE\"\n",
        "human_seq = \"MAMQMQLEANADTSVEEESFGPQPISRLEQCGINANDVKKLEEAGFHTVEAVAYAPKKELINIKGISEAKADKILAEAAKLVPMGFTTATEFHQRRSEIIQITTGSKELDKLLQGGIETGSITEMFGEFRTGKTQICHTLAVTCQLPIDRGGGEGKAMYIDTEGTFRPERLLAVAERYGLSGSDVLDNVAYARAFNTDHQTQLLYQASAMMVESRYALLIVDSATALYRTDYSGRGELSARQMHLARFLRMLLRLADEFGVAVVITNQVVAQVDGAAMFAADPKKPIGGNIIAHASTTRLYLRKGRGETRICKIYDSPCLPEAEAMFAINADGVGDAKD\"\n",
        "\n",
        "\n",
        "#query_seq = \">Query\\n\" + yeast_Rad51\n",
        "query_seq = \">Query\\n\" + human_seq\n",
        "\n",
        "for order in orders:\n",
        "  #### SET UP LIST OF ORDERS FROM SPREADSHEET ANALYSIS\n",
        "  order_query = order + \"[ORGN]\"\n",
        "\n",
        "  # Perform pBLAST search\n",
        "  result = NCBIWWW.qblast(\"blastp\", \"nr\", query_seq, entrez_query=order_query, hitlist_size=10)\n",
        "\n",
        "  # Parse the BLAST results\n",
        "  rad51_parsed = NCBIXML.parse(result)\n",
        "\n",
        "  # Extract information from XML parse\n",
        "  for record in rad51_parsed:\n",
        "    order_name = []\n",
        "    title = []\n",
        "    length = []\n",
        "    subject = []\n",
        "\n",
        "    for alignment in record.alignments:\n",
        "      t = alignment.title\n",
        "\n",
        "      order_name.append(order)\n",
        "      title.append(t)\n",
        "      length.append(alignment.hsps[0].align_length)\n",
        "\n",
        "      # Parses accession number and uses Entrez to fetch protein sequence\n",
        "      index = t.find(\"|\")\n",
        "      accession_number = t[index+1 : t[index+1:].find('|') + index+1]\n",
        "      protein_sequence = fetch_protein_sequence(accession_number)\n",
        "      subject.append(protein_sequence)\n",
        "\n",
        "    data = {\n",
        "          \"Order\": order_name,\n",
        "          \"Title\": title,\n",
        "          \"Alignment Length\": length,\n",
        "          \"Alignment Sequence\": subject\n",
        "      }\n",
        "    df = pd.DataFrame(data)\n",
        "    master = pd.concat([df,master], ignore_index=True)\n",
        "    master.to_csv(path + \"metazoa_data_nov.csv\", index=False)\n",
        "\n",
        "    print(order + \" done...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0jueMQestR2"
      },
      "outputs": [],
      "source": [
        "order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kb2pHPPqggt"
      },
      "outputs": [],
      "source": [
        "master.to_csv(path + \"/nov_data/fungi_data_nov.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}